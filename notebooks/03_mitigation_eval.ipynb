{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mitigation Evaluation\n",
    "\n",
    "This notebook evaluates debiasing techniques for mitigating dataset artifacts.\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from datasets import load_dataset\n",
    "from mitigation import EnsembleDebiaser, DatasetCartographer, AdversarialTrainer\n",
    "from analysis import ErrorAnalyzer\n",
    "from analysis.visualization import create_comparison_table\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose Mitigation Strategy\n",
    "\n",
    "Options:\n",
    "1. **Dataset Cartography**: Focus on hard examples\n",
    "2. **Ensemble Debiasing**: Train biased model and debias main model\n",
    "3. **Adversarial Training**: Augment with adversarial examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1: Dataset Cartography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize cartographer\n",
    "cartographer = DatasetCartographer(\n",
    "    model_path='google/electra-small-discriminator',\n",
    "    dataset_name='snli'\n",
    ")\n",
    "\n",
    "# This would require full training loop integration\n",
    "# See mitigation/dataset_cartography.py for full implementation\n",
    "print(\"Dataset Cartography analysis ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2: Ensemble Debiasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset('snli')['train']\n",
    "dataset = dataset.filter(lambda x: x['label'] != -1)\n",
    "dataset = dataset.select(range(1000))  # Use subset for demo\n",
    "\n",
    "# Initialize debiaser\n",
    "debiaser = EnsembleDebiaser(\n",
    "    main_model_path='google/electra-small-discriminator',\n",
    "    num_labels=3\n",
    ")\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "print(\"Ensemble debiaser initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Train biased model (hypothesis-only)\n",
    "debiaser.train_biased_model(\n",
    "    dataset=dataset,\n",
    "    output_dir='../models/biased_snli',\n",
    "    num_epochs=2,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "print(\"Biased model training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Compute example weights\n",
    "weights = debiaser.compute_example_weights(dataset, temperature=1.0)\n",
    "\n",
    "print(f\"Computed weights for {len(weights)} examples\")\n",
    "print(f\"Mean weight: {weights.mean():.3f}\")\n",
    "print(f\"Std weight: {weights.std():.3f}\")\n",
    "\n",
    "# Visualize weight distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(weights, bins=30, edgecolor='black')\n",
    "plt.xlabel('Example Weight')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Example Weights')\n",
    "plt.axvline(weights.mean(), color='red', linestyle='--', label='Mean')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Train debiased model with reweighted examples\n",
    "debiaser.train_debiased_model(\n",
    "    dataset=dataset,\n",
    "    output_dir='../models/debiased_snli',\n",
    "    example_weights=weights,\n",
    "    num_epochs=3,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "print(\"Debiased model training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 3: Adversarial Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize adversarial trainer\n",
    "adv_trainer = AdversarialTrainer(\n",
    "    model_path='google/electra-small-discriminator',\n",
    "    num_labels=3\n",
    ")\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset('snli')['train']\n",
    "dataset = dataset.filter(lambda x: x['label'] != -1)\n",
    "dataset = dataset.select(range(1000))\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with adversarial augmentation\n",
    "adv_trainer.train_with_adversarial_examples(\n",
    "    dataset=dataset,\n",
    "    output_dir='../models/adversarial_snli',\n",
    "    augmentation_ratio=0.3,  # Add 30% adversarial examples\n",
    "    num_epochs=3,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "print(\"Adversarial training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Mitigated Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate baseline\n",
    "baseline_analyzer = ErrorAnalyzer('../models/baseline_snli')\n",
    "baseline_results = baseline_analyzer.analyze_dataset(\n",
    "    dataset_name='snli',\n",
    "    split='validation',\n",
    "    max_samples=1000\n",
    ")\n",
    "\n",
    "print(f\"Baseline accuracy: {baseline_results['accuracy']:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate mitigated model\n",
    "mitigated_analyzer = ErrorAnalyzer('../models/debiased_snli')  # or adversarial_snli\n",
    "mitigated_results = mitigated_analyzer.analyze_dataset(\n",
    "    dataset_name='snli',\n",
    "    split='validation',\n",
    "    max_samples=1000\n",
    ")\n",
    "\n",
    "print(f\"Mitigated accuracy: {mitigated_results['accuracy']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "comparison = create_comparison_table(baseline_results, mitigated_results)\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(comparison.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test hypothesis-only baseline for both models\n",
    "baseline_hyp = baseline_analyzer.analyze_hypothesis_only(max_samples=1000)\n",
    "mitigated_hyp = mitigated_analyzer.analyze_hypothesis_only(max_samples=1000)\n",
    "\n",
    "print(\"\\nHypothesis-only Accuracy:\")\n",
    "print(f\"  Baseline: {baseline_hyp['hypothesis_only_accuracy']:.2%}\")\n",
    "print(f\"  Mitigated: {mitigated_hyp['hypothesis_only_accuracy']:.2%}\")\n",
    "\n",
    "# Visualize comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "models = ['Baseline', 'Mitigated']\n",
    "full_acc = [baseline_results['accuracy'], mitigated_results['accuracy']]\n",
    "hyp_acc = [baseline_hyp['hypothesis_only_accuracy'], mitigated_hyp['hypothesis_only_accuracy']]\n",
    "\n",
    "x = np.arange(len(models))\n",
    "width = 0.35\n",
    "\n",
    "ax.bar(x - width/2, full_acc, width, label='Full Model', color='skyblue')\n",
    "ax.bar(x + width/2, hyp_acc, width, label='Hypothesis-Only', color='lightcoral')\n",
    "\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Model Comparison: Full vs Hypothesis-Only')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(models)\n",
    "ax.legend()\n",
    "ax.set_ylim([0, 1])\n",
    "\n",
    "# Add value labels\n",
    "for i, (f, h) in enumerate(zip(full_acc, hyp_acc)):\n",
    "    ax.text(i - width/2, f + 0.02, f'{f:.2%}', ha='center', fontweight='bold')\n",
    "    ax.text(i + width/2, h + 0.02, f'{h:.2%}', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Key metrics to evaluate:\n",
    "1. **Overall accuracy**: Did it improve?\n",
    "2. **Hypothesis-only accuracy**: Did it decrease (less artifact exploitation)?\n",
    "3. **Error patterns**: Are errors more evenly distributed?\n",
    "4. **Robustness**: Performance on hard/adversarial examples?\n",
    "\n",
    "A successful mitigation should:\n",
    "- Maintain or improve overall accuracy\n",
    "- Reduce hypothesis-only baseline performance\n",
    "- Improve performance on hard/out-of-distribution examples"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
