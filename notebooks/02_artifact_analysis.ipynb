{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artifact Analysis\n",
    "\n",
    "This notebook analyzes the baseline model to identify dataset artifacts and spurious correlations.\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from analysis import ErrorAnalyzer, ContrastSetEvaluator\n",
    "from analysis.visualization import (\n",
    "    plot_error_types,\n",
    "    plot_confidence_distribution,\n",
    "    plot_contrast_set_results\n",
    ")\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '../models/baseline_snli'\n",
    "\n",
    "analyzer = ErrorAnalyzer(model_path)\n",
    "print(f\"Model loaded from: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis\n",
    "\n",
    "Analyze model errors on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze predictions\n",
    "results = analyzer.analyze_dataset(\n",
    "    dataset_name='snli',\n",
    "    split='validation',\n",
    "    max_samples=1000\n",
    ")\n",
    "\n",
    "print(f\"\\nOverall Accuracy: {results['accuracy']:.2%}\")\n",
    "print(f\"Total Errors: {results['errors']}\")\n",
    "print(f\"Error Rate: {results['error_rate']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Error Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot error distribution\n",
    "plot_error_types(results)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze confidence distribution\n",
    "plot_confidence_distribution(analyzer.predictions)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis-Only Baseline\n",
    "\n",
    "Test if the model exploits hypothesis-only biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run hypothesis-only analysis\n",
    "hyp_results = analyzer.analyze_hypothesis_only(max_samples=1000)\n",
    "\n",
    "print(f\"Hypothesis-only accuracy: {hyp_results['hypothesis_only_accuracy']:.2%}\")\n",
    "print(f\"Full model accuracy: {results['accuracy']:.2%}\")\n",
    "print(f\"\\nGap: {(results['accuracy'] - hyp_results['hypothesis_only_accuracy']):.2%}\")\n",
    "\n",
    "if hyp_results['hypothesis_only_accuracy'] > 0.4:\n",
    "    print(\"\\n⚠️ Warning: High hypothesis-only accuracy suggests dataset artifacts!\")\n",
    "    print(\"Random baseline would be 33.3% for 3-class NLI.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine High-Confidence Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at high-confidence errors (model is very confident but wrong)\n",
    "error_patterns = results['error_patterns']\n",
    "high_conf_errors = error_patterns['high_confidence_errors']\n",
    "\n",
    "print(f\"High-confidence errors: {len(high_conf_errors)}\")\n",
    "print(\"\\nExamples of high-confidence errors:\\n\")\n",
    "\n",
    "for i, error in enumerate(high_conf_errors[:5]):\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"  Premise: {error['premise']}\")\n",
    "    print(f\"  Hypothesis: {error['hypothesis']}\")\n",
    "    print(f\"  True label: {error['true_label']}, Predicted: {error['pred_label']}\")\n",
    "    print(f\"  Confidence: {error['confidence']:.2%}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexical Overlap Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis.error_analysis import find_lexical_overlaps\n",
    "\n",
    "# Analyze lexical overlap for errors\n",
    "overlap_data = []\n",
    "\n",
    "for error in analyzer.errors[:100]:\n",
    "    overlap = find_lexical_overlaps(error['premise'], error['hypothesis'])\n",
    "    overlap_data.append({\n",
    "        'overlap_ratio': overlap['overlap_ratio'],\n",
    "        'true_label': error['true_label'],\n",
    "        'pred_label': error['pred_label'],\n",
    "    })\n",
    "\n",
    "overlap_df = pd.DataFrame(overlap_data)\n",
    "\n",
    "# Plot overlap distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(overlap_df['overlap_ratio'], bins=20, edgecolor='black')\n",
    "plt.xlabel('Lexical Overlap Ratio')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Lexical Overlap Distribution in Errors')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Mean overlap ratio: {overlap_df['overlap_ratio'].mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export errors for further analysis\n",
    "analyzer.export_errors('../results/analysis/errors.csv')\n",
    "print(\"Errors exported to ../results/analysis/errors.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Key findings:\n",
    "1. Overall accuracy and error rate\n",
    "2. Hypothesis-only baseline performance (indicates artifacts)\n",
    "3. Error patterns by label type\n",
    "4. High-confidence errors (potential systematic biases)\n",
    "5. Lexical overlap patterns in errors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
