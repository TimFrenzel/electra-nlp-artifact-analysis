{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELECTRA NLP Artifact Analysis - Training Notebook\n",
    "\n",
    "**Professional Research Project**: Investigating dataset artifacts in NLP models\n",
    "\n",
    "**Environment**: Google Colab with GPU (A100 recommended)\n",
    "\n",
    "**Objective**: Train baseline ELECTRA-small model on SNLI for artifact analysis\n",
    "\n",
    "---\n",
    "\n",
    "## Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f WARNING: No GPU detected. Training will be slow.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mount Google Drive\n",
    "\n",
    "Mount Drive to save checkpoints and results persistently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Set up project directory\n",
    "import os\n",
    "PROJECT_DIR = '/content/drive/MyDrive/electra-artifact-analysis'\n",
    "os.makedirs(PROJECT_DIR, exist_ok=True)\n",
    "os.makedirs(f\"{PROJECT_DIR}/models\", exist_ok=True)\n",
    "os.makedirs(f\"{PROJECT_DIR}/results\", exist_ok=True)\n",
    "os.makedirs(f\"{PROJECT_DIR}/logs\", exist_ok=True)\n",
    "\n",
    "print(f\"\u2713 Project directory: {PROJECT_DIR}\")\n",
    "print(f\"\u2713 All subdirectories created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clone Repository & Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone your repository\n",
    "!git clone https://github.com/TimFrenzel/electra-nlp-artifact-analysis.git /content/electra-nlp-artifact-analysis\n",
    "%cd /content/electra-nlp-artifact-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install --upgrade pip\n",
    "!pip install -q -r requirements.txt\n",
    "\n",
    "# Verify installations\n",
    "import transformers\n",
    "import datasets\n",
    "import evaluate\n",
    "\n",
    "print(f\"\u2713 Transformers: {transformers.__version__}\")\n",
    "print(f\"\u2713 Datasets: {datasets.__version__}\")\n",
    "print(f\"\u2713 Evaluate: {evaluate.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set hyperparameters and paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "CONFIG = {\n",
    "    'task': 'nli',\n",
    "    'dataset': 'snli',\n",
    "    'model': 'google/electra-small-discriminator',\n",
    "    'num_train_epochs': 3,\n",
    "    'batch_size': 32,  # Adjust based on GPU memory\n",
    "    'learning_rate': 2e-5,\n",
    "    'max_seq_length': 128,\n",
    "    'save_steps': 500,\n",
    "    'eval_steps': 500,\n",
    "    'output_dir': f\"{PROJECT_DIR}/models/baseline_snli\",\n",
    "    'seed': 42,\n",
    "}\n",
    "\n",
    "# Display configuration\n",
    "import json\n",
    "print(\"Training Configuration:\")\n",
    "print(json.dumps(CONFIG, indent=2))\n",
    "\n",
    "# Save configuration\n",
    "with open(f\"{PROJECT_DIR}/logs/training_config.json\", 'w') as f:\n",
    "    json.dump(CONFIG, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Baseline Training\n",
    "\n",
    "Train baseline ELECTRA-small on SNLI.\n",
    "\n",
    "**Expected Performance**: ~89% accuracy (3 epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test on small subset first\n",
    "print(\"\ud83e\uddea Quick test on 1000 examples...\")\n",
    "\n",
    "# Extract config values for cleaner shell command syntax\n",
    "task = CONFIG['task']\n",
    "dataset = CONFIG['dataset']\n",
    "model = CONFIG['model']\n",
    "batch_size = CONFIG['batch_size']\n",
    "max_seq_length = CONFIG['max_seq_length']\n",
    "seed = CONFIG['seed']\n",
    "\n",
    "!python run.py \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --task {{task}} \\\n",
    "    --dataset {{dataset}} \\\n",
    "    --model {{model}} \\\n",
    "    --output_dir {{PROJECT_DIR}}/models/debug \\\n",
    "    --num_train_epochs 1 \\\n",
    "    --batch_size {{batch_size}} \\\n",
    "    --max_train_samples 1000 \\\n",
    "    --max_eval_samples 500 \\\n",
    "    --max_seq_length {{max_seq_length}} \\\n",
    "    --seed {{seed}}\n",
    "\n",
    "print(\"\u2713 Quick test complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full training on complete dataset\n",
    "print(\"\ud83d\ude80 Starting full training...\")\n",
    "print(f\"   Dataset: {CONFIG['dataset'].upper()}\")\n",
    "print(f\"   Model: {CONFIG['model']}\")\n",
    "print(f\"   Epochs: {CONFIG['num_train_epochs']}\")\n",
    "print(f\"   Expected time: 1-3 hours with A100\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Extract config values for cleaner shell command syntax\n",
    "task = CONFIG['task']\n",
    "dataset = CONFIG['dataset']\n",
    "model = CONFIG['model']\n",
    "output_dir = CONFIG['output_dir']\n",
    "num_epochs = CONFIG['num_train_epochs']\n",
    "batch_size = CONFIG['batch_size']\n",
    "lr = CONFIG['learning_rate']\n",
    "max_seq_length = CONFIG['max_seq_length']\n",
    "save_steps = CONFIG['save_steps']\n",
    "eval_steps = CONFIG['eval_steps']\n",
    "seed = CONFIG['seed']\n",
    "\n",
    "!python run.py \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --task {task} \\\n",
    "    --dataset {dataset} \\\n",
    "    --model {model} \\\n",
    "    --output_dir {output_dir} \\\n",
    "    --num_train_epochs {num_epochs} \\\n",
    "    --batch_size {batch_size} \\\n",
    "    --learning_rate {lr} \\\n",
    "    --max_seq_length {max_seq_length} \\\n",
    "    --save_steps {save_steps} \\\n",
    "    --eval_steps {eval_steps} \\\n",
    "    --seed {seed} \\\n",
    "    --fp16\n",
    "\n",
    "print(\"\\n\u2713 Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Baseline Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on full validation set\n",
    "task = CONFIG['task']\n",
    "dataset = CONFIG['dataset']\n",
    "output_dir = CONFIG['output_dir']\n",
    "batch_size = CONFIG['batch_size']\n",
    "\n",
    "!python run.py \\\n",
    "    --do_eval \\\n",
    "    --task {task} \\\n",
    "    --dataset {dataset} \\\n",
    "    --model {output_dir} \\\n",
    "    --output_dir {PROJECT_DIR}/results/baseline \\\n",
    "    --batch_size {batch_size}\n",
    "\n",
    "# Load and display results\n",
    "import json\n",
    "\n",
    "with open(f\"{PROJECT_DIR}/results/baseline/eval_results.json\", 'r') as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BASELINE PERFORMANCE\")\n",
    "print(\"=\"*60)\n",
    "for key, value in results.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"{key:30s}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"{key:30s}: {value}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check against expected performance\n",
    "if 'eval_accuracy' in results:\n",
    "    acc = results['eval_accuracy']\n",
    "    if acc >= 0.89:\n",
    "        print(f\"\u2713 EXCELLENT: Accuracy {acc:.2%} meets/exceeds expected ~89%\")\n",
    "    elif acc >= 0.85:\n",
    "        print(f\"\u2713 GOOD: Accuracy {acc:.2%} is reasonable (target: ~89%)\")\n",
    "    else:\n",
    "        print(f\"\u26a0\ufe0f WARNING: Accuracy {acc:.2%} below expected ~89%. Check training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Training Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training summary\n",
    "import datetime\n",
    "\n",
    "summary = {\n",
    "    'timestamp': datetime.datetime.now().isoformat(),\n",
    "    'config': CONFIG,\n",
    "    'results': results,\n",
    "    'gpu': torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU',\n",
    "    'model_path': CONFIG['output_dir'],\n",
    "    'notes': [\n",
    "        'Baseline training complete',\n",
    "        'Ready for artifact analysis (Part 1)',\n",
    "        'Checkpoints saved to Google Drive'\n",
    "    ]\n",
    "}\n",
    "\n",
    "summary_path = f\"{PROJECT_DIR}/logs/training_summary.json\"\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"\u2713 Training summary saved to: {summary_path}\")\n",
    "print(\"\\n\ud83d\udccb Next Steps:\")\n",
    "print(\"   1. Run analysis notebook (Part 1: Artifact Analysis)\")\n",
    "print(\"   2. Identify dataset artifacts and spurious correlations\")\n",
    "print(\"   3. Design mitigation strategy (Part 2)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Quick Artifact Check\n",
    "\n",
    "Quick hypothesis-only baseline to check for artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a preview - full analysis in separate notebook\n",
    "from analysis.error_analysis import ErrorAnalyzer\n",
    "\n",
    "print(\"\ud83d\udd0d Quick artifact check (hypothesis-only baseline)...\")\n",
    "print(\"   Random baseline: 33.3% (3-class classification)\")\n",
    "print(\"   Biased baseline: ~67% (indicates artifacts)\")\n",
    "print(\"   Full model: ~89%\\n\")\n",
    "\n",
    "analyzer = ErrorAnalyzer(CONFIG['output_dir'])\n",
    "hyp_results = analyzer.analyze_hypothesis_only(max_samples=1000)\n",
    "\n",
    "hyp_acc = hyp_results['hypothesis_only_accuracy']\n",
    "print(f\"\\nHypothesis-only accuracy: {hyp_acc:.2%}\")\n",
    "\n",
    "if hyp_acc > 0.6:\n",
    "    print(\"\u26a0\ufe0f HIGH ARTIFACT SIGNAL: Model likely exploits hypothesis-only biases\")\n",
    "    print(\"   \u2192 This confirms need for debiasing (Part 2)\")\n",
    "elif hyp_acc > 0.5:\n",
    "    print(\"\u26a0\ufe0f MODERATE ARTIFACT SIGNAL: Some hypothesis-only bias present\")\n",
    "else:\n",
    "    print(\"\u2713 LOW ARTIFACT SIGNAL: Model relies on full context\")\n",
    "\n",
    "print(\"\\nFor full artifact analysis, see: analysis_part1.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Training Complete \u2713\n",
    "\n",
    "### Files Saved to Google Drive:\n",
    "- Model checkpoints: `{PROJECT_DIR}/models/baseline_snli/`\n",
    "- Evaluation results: `{PROJECT_DIR}/results/baseline/`\n",
    "- Training logs: `{PROJECT_DIR}/logs/`\n",
    "\n",
    "### Next Notebooks:\n",
    "1. **analysis_part1.ipynb** - Artifact analysis and error characterization\n",
    "2. **mitigation_part2.ipynb** - Implement debiasing methods\n",
    "\n",
    "### For Report:\n",
    "- Baseline accuracy: Record in Part 1\n",
    "- Training details: Include in Method section\n",
    "- Hypothesis-only baseline: Include in Analysis section"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}