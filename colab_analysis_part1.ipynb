{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Artifact Analysis - ELECTRA on SNLI\n",
    "\n",
    "**Professional Research Project**: Systematic identification and characterization of dataset artifacts\n",
    "\n",
    "**Environment**: Google Colab with GPU (A100 recommended)\n",
    "\n",
    "**Prerequisite**: Complete baseline training (colab_training.ipynb)\n",
    "\n",
    "**Objective**: Identify spurious correlations and dataset biases that models exploit\n",
    "\n",
    "---\n",
    "\n",
    "## Analysis Components\n",
    "\n",
    "1. **Hypothesis-Only Baseline** - Test if model relies on hypothesis-only bias (~67% vs 33% random)\n",
    "2. **Error Characterization** - Systematic analysis of model failures\n",
    "3. **Lexical Overlap Analysis** - Correlation between word overlap and predictions\n",
    "4. **Statistical Artifact Detection** - Length bias, word frequency patterns\n",
    "5. **Contrast Sets** - Robustness to minimal perturbations\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è WARNING: No GPU detected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "PROJECT_DIR = '/content/drive/MyDrive/electra-artifact-analysis'\n",
    "\n",
    "# Verify baseline model exists\n",
    "BASELINE_MODEL = f\"{PROJECT_DIR}/models/baseline_snli\"\n",
    "if not os.path.exists(BASELINE_MODEL):\n",
    "    print(\"‚ùå ERROR: Baseline model not found!\")\n",
    "    print(f\"   Expected: {BASELINE_MODEL}\")\n",
    "    print(\"   ‚Üí Run colab_training.ipynb first\")\n",
    "else:\n",
    "    print(f\"‚úì Baseline model found: {BASELINE_MODEL}\")\n",
    "\n",
    "# Create analysis output directory\n",
    "os.makedirs(f\"{PROJECT_DIR}/analysis_results\", exist_ok=True)\n",
    "os.makedirs(f\"{PROJECT_DIR}/figures\", exist_ok=True)\n",
    "print(\"‚úì Analysis directories ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clone Repository & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository\n",
    "!git clone https://github.com/TimFrenzel/electra-nlp-artifact-analysis.git /content/electra-nlp-artifact-analysis\n",
    "%cd /content/electra-nlp-artifact-analysis\n",
    "\n",
    "# Install dependencies\n",
    "!pip install -q -r requirements.txt\n",
    "\n",
    "print(\"‚úì Repository cloned and dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.1: Hypothesis-Only Baseline\n",
    "\n",
    "**Research Question**: Does the model exploit hypothesis-only bias?\n",
    "\n",
    "**Expected Results**:\n",
    "- Random baseline: 33.3% (3-class classification)\n",
    "- Biased baseline: ~67% (indicates severe artifacts)\n",
    "- Full model: ~89%\n",
    "\n",
    "**Interpretation**: If hypothesis-only > 60%, model likely exploits spurious correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load SNLI dataset\n",
    "print(\"Loading SNLI dataset...\")\n",
    "snli = load_dataset(\"snli\")\n",
    "\n",
    "# Filter invalid labels\n",
    "snli_valid = snli.filter(lambda x: x[\"label\"] != -1)\n",
    "print(f\"‚úì Dataset loaded: {len(snli_valid['validation'])} validation examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train hypothesis-only model\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"HYPOTHESIS-ONLY BASELINE TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Prepare hypothesis-only dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/electra-small-discriminator\")\n",
    "\n",
    "def tokenize_hypothesis_only(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"hypothesis\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "# Tokenize\n",
    "train_hyp_only = snli_valid[\"train\"].map(tokenize_hypothesis_only, batched=True)\n",
    "eval_hyp_only = snli_valid[\"validation\"].map(tokenize_hypothesis_only, batched=True)\n",
    "\n",
    "# Format for PyTorch\n",
    "train_hyp_only = train_hyp_only.rename_column(\"label\", \"labels\")\n",
    "eval_hyp_only = eval_hyp_only.rename_column(\"label\", \"labels\")\n",
    "train_hyp_only.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "eval_hyp_only.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "print(f\"‚úì Hypothesis-only dataset prepared\")\n",
    "print(f\"   Train: {len(train_hyp_only)} examples\")\n",
    "print(f\"   Eval: {len(eval_hyp_only)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train hypothesis-only model\n",
    "hyp_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"google/electra-small-discriminator\",\n",
    "    num_labels=3\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"{PROJECT_DIR}/models/hypothesis_only\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=True,\n",
    "    logging_steps=100,\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": accuracy_score(labels, predictions)}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=hyp_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_hyp_only,\n",
    "    eval_dataset=eval_hyp_only,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"\\nüöÄ Training hypothesis-only model...\")\n",
    "print(\"   Expected time: 30-60 minutes with A100\\n\")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n‚úì Hypothesis-only training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate hypothesis-only model\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"HYPOTHESIS-ONLY BASELINE RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "hyp_results = trainer.evaluate()\n",
    "hyp_accuracy = hyp_results[\"eval_accuracy\"]\n",
    "\n",
    "print(f\"\\nHypothesis-only accuracy: {hyp_accuracy:.2%}\")\n",
    "print(f\"Random baseline: 33.3%\")\n",
    "print(f\"Expected biased baseline: ~67%\")\n",
    "\n",
    "# Interpret results\n",
    "if hyp_accuracy >= 0.65:\n",
    "    artifact_severity = \"SEVERE\"\n",
    "    color = \"üî¥\"\n",
    "    interpretation = \"Model heavily exploits hypothesis-only bias. Strong artifacts present.\"\n",
    "elif hyp_accuracy >= 0.55:\n",
    "    artifact_severity = \"MODERATE\"\n",
    "    color = \"üü°\"\n",
    "    interpretation = \"Moderate hypothesis-only bias detected. Some artifacts present.\"\n",
    "elif hyp_accuracy >= 0.45:\n",
    "    artifact_severity = \"MILD\"\n",
    "    color = \"üü¢\"\n",
    "    interpretation = \"Mild hypothesis-only bias. Model uses some context.\"\n",
    "else:\n",
    "    artifact_severity = \"MINIMAL\"\n",
    "    color = \"‚úÖ\"\n",
    "    interpretation = \"Minimal hypothesis-only bias. Model relies on full context.\"\n",
    "\n",
    "print(f\"\\n{color} ARTIFACT SEVERITY: {artifact_severity}\")\n",
    "print(f\"   {interpretation}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Save results\n",
    "import json\n",
    "hyp_summary = {\n",
    "    \"hypothesis_only_accuracy\": hyp_accuracy,\n",
    "    \"random_baseline\": 0.333,\n",
    "    \"artifact_severity\": artifact_severity,\n",
    "    \"interpretation\": interpretation,\n",
    "}\n",
    "\n",
    "with open(f\"{PROJECT_DIR}/analysis_results/hypothesis_only_results.json\", 'w') as f:\n",
    "    json.dump(hyp_summary, f, indent=2)\n",
    "\n",
    "print(f\"\\n‚úì Results saved to: {PROJECT_DIR}/analysis_results/hypothesis_only_results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.2: Per-Class Hypothesis-Only Analysis\n",
    "\n",
    "Analyze which classes are most predictable from hypothesis alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Get predictions\n",
    "predictions = trainer.predict(eval_hyp_only)\n",
    "pred_labels = np.argmax(predictions.predictions, axis=1)\n",
    "true_labels = predictions.label_ids\n",
    "\n",
    "# Classification report\n",
    "label_names = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "print(\"\\nPer-Class Performance (Hypothesis-Only):\")\n",
    "print(classification_report(true_labels, pred_labels, target_names=label_names))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(true_labels, pred_labels)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_normalized, annot=True, fmt=\".2%\", cmap=\"YlOrRd\", \n",
    "            xticklabels=label_names, yticklabels=label_names)\n",
    "plt.title(\"Hypothesis-Only Confusion Matrix (Normalized)\", fontsize=14, weight='bold')\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{PROJECT_DIR}/figures/hypothesis_only_confusion_matrix.png\", dpi=300)\n",
    "print(f\"\\n‚úì Confusion matrix saved to figures/hypothesis_only_confusion_matrix.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.3: Lexical Overlap Analysis\n",
    "\n",
    "**Hypothesis**: High word overlap between premise and hypothesis correlates with entailment predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load baseline model predictions\n",
    "baseline_tokenizer = AutoTokenizer.from_pretrained(BASELINE_MODEL)\n",
    "baseline_model = AutoModelForSequenceClassification.from_pretrained(BASELINE_MODEL)\n",
    "\n",
    "# Prepare full context dataset\n",
    "def tokenize_full_context(examples):\n",
    "    return baseline_tokenizer(\n",
    "        examples[\"premise\"],\n",
    "        examples[\"hypothesis\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "eval_full = snli_valid[\"validation\"].map(tokenize_full_context, batched=True)\n",
    "eval_full = eval_full.rename_column(\"label\", \"labels\")\n",
    "eval_full.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "# Get baseline predictions\n",
    "baseline_trainer = Trainer(\n",
    "    model=baseline_model,\n",
    "    args=TrainingArguments(output_dir=\"/tmp\", per_device_eval_batch_size=32),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "baseline_predictions = baseline_trainer.predict(eval_full)\n",
    "baseline_pred_labels = np.argmax(baseline_predictions.predictions, axis=1)\n",
    "\n",
    "print(f\"‚úì Baseline model predictions obtained\")\n",
    "print(f\"   Baseline accuracy: {accuracy_score(true_labels, baseline_pred_labels):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute lexical overlap for each example\n",
    "def compute_lexical_overlap(premise, hypothesis):\n",
    "    \"\"\"Compute Jaccard similarity (word overlap)\"\"\"\n",
    "    p_words = set(premise.lower().split())\n",
    "    h_words = set(hypothesis.lower().split())\n",
    "    \n",
    "    if len(h_words) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    overlap = len(p_words & h_words)\n",
    "    return overlap / len(h_words)\n",
    "\n",
    "# Calculate overlap for validation set\n",
    "val_examples = snli_valid[\"validation\"]\n",
    "overlaps = []\n",
    "\n",
    "for i in range(len(val_examples)):\n",
    "    overlap = compute_lexical_overlap(\n",
    "        val_examples[i][\"premise\"],\n",
    "        val_examples[i][\"hypothesis\"]\n",
    "    )\n",
    "    overlaps.append(overlap)\n",
    "\n",
    "overlaps = np.array(overlaps)\n",
    "print(f\"‚úì Lexical overlap computed for {len(overlaps)} examples\")\n",
    "print(f\"   Mean overlap: {overlaps.mean():.2%}\")\n",
    "print(f\"   Std overlap: {overlaps.std():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze correlation between overlap and predictions\n",
    "overlap_df = pd.DataFrame({\n",
    "    'overlap': overlaps,\n",
    "    'true_label': true_labels,\n",
    "    'baseline_pred': baseline_pred_labels,\n",
    "    'baseline_correct': (baseline_pred_labels == true_labels).astype(int)\n",
    "})\n",
    "\n",
    "# Stratify by overlap level\n",
    "overlap_df['overlap_bin'] = pd.cut(overlap_df['overlap'], \n",
    "                                     bins=[0, 0.2, 0.4, 0.6, 0.8, 1.0],\n",
    "                                     labels=['0-20%', '20-40%', '40-60%', '60-80%', '80-100%'])\n",
    "\n",
    "# Accuracy by overlap bin\n",
    "accuracy_by_overlap = overlap_df.groupby('overlap_bin')['baseline_correct'].mean()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LEXICAL OVERLAP ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nAccuracy by Lexical Overlap Level:\")\n",
    "print(accuracy_by_overlap.to_string())\n",
    "print(\"\\nInterpretation:\")\n",
    "if accuracy_by_overlap.iloc[-1] > accuracy_by_overlap.iloc[0] + 0.1:\n",
    "    print(\"‚ö†Ô∏è STRONG LEXICAL OVERLAP BIAS: High overlap ‚Üí higher accuracy\")\n",
    "    print(\"   Model likely exploits superficial word matching\")\n",
    "else:\n",
    "    print(\"‚úì MINIMAL LEXICAL OVERLAP BIAS: Model not over-relying on overlap\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize overlap distribution by label\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Overlap distribution by true label\n",
    "for label_idx, label_name in enumerate(label_names):\n",
    "    label_overlaps = overlaps[true_labels == label_idx]\n",
    "    axes[0].hist(label_overlaps, bins=20, alpha=0.6, label=label_name)\n",
    "    \n",
    "axes[0].set_xlabel(\"Lexical Overlap (Jaccard Similarity)\")\n",
    "axes[0].set_ylabel(\"Frequency\")\n",
    "axes[0].set_title(\"Lexical Overlap Distribution by True Label\", weight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Plot 2: Accuracy by overlap bin\n",
    "accuracy_by_overlap.plot(kind='bar', ax=axes[1], color='steelblue')\n",
    "axes[1].set_xlabel(\"Lexical Overlap Bin\")\n",
    "axes[1].set_ylabel(\"Accuracy\")\n",
    "axes[1].set_title(\"Model Accuracy vs. Lexical Overlap\", weight='bold')\n",
    "axes[1].set_xticklabels(axes[1].get_xticklabels(), rotation=45)\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "axes[1].axhline(y=accuracy_by_overlap.mean(), color='red', linestyle='--', label='Mean Accuracy')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{PROJECT_DIR}/figures/lexical_overlap_analysis.png\", dpi=300)\n",
    "print(f\"\\n‚úì Overlap analysis saved to figures/lexical_overlap_analysis.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.4: Length Bias Analysis\n",
    "\n",
    "Test if hypothesis length correlates with predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute hypothesis lengths\n",
    "hyp_lengths = [len(val_examples[i][\"hypothesis\"].split()) for i in range(len(val_examples))]\n",
    "hyp_lengths = np.array(hyp_lengths)\n",
    "\n",
    "length_df = pd.DataFrame({\n",
    "    'length': hyp_lengths,\n",
    "    'true_label': true_labels,\n",
    "    'baseline_pred': baseline_pred_labels,\n",
    "    'baseline_correct': (baseline_pred_labels == true_labels).astype(int)\n",
    "})\n",
    "\n",
    "# Stratify by length\n",
    "length_df['length_bin'] = pd.cut(length_df['length'],\n",
    "                                  bins=[0, 5, 10, 15, 100],\n",
    "                                  labels=['1-5', '6-10', '11-15', '16+'])\n",
    "\n",
    "# Accuracy by length\n",
    "accuracy_by_length = length_df.groupby('length_bin')['baseline_correct'].mean()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LENGTH BIAS ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nAccuracy by Hypothesis Length (words):\")\n",
    "print(accuracy_by_length.to_string())\n",
    "\n",
    "# Label distribution by length\n",
    "print(\"\\nLabel Distribution by Length:\")\n",
    "label_dist_by_length = pd.crosstab(length_df['length_bin'], length_df['true_label'], normalize='index')\n",
    "label_dist_by_length.columns = label_names\n",
    "print(label_dist_by_length.to_string())\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize length bias\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Label distribution by length\n",
    "label_dist_by_length.plot(kind='bar', stacked=True, ax=axes[0], \n",
    "                          color=['#2ecc71', '#f39c12', '#e74c3c'])\n",
    "axes[0].set_xlabel(\"Hypothesis Length (words)\")\n",
    "axes[0].set_ylabel(\"Proportion\")\n",
    "axes[0].set_title(\"Label Distribution by Hypothesis Length\", weight='bold')\n",
    "axes[0].set_xticklabels(axes[0].get_xticklabels(), rotation=45)\n",
    "axes[0].legend(title=\"Label\")\n",
    "\n",
    "# Plot 2: Accuracy by length\n",
    "accuracy_by_length.plot(kind='bar', ax=axes[1], color='coral')\n",
    "axes[1].set_xlabel(\"Hypothesis Length (words)\")\n",
    "axes[1].set_ylabel(\"Accuracy\")\n",
    "axes[1].set_title(\"Model Accuracy by Hypothesis Length\", weight='bold')\n",
    "axes[1].set_xticklabels(axes[1].get_xticklabels(), rotation=45)\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "axes[1].axhline(y=accuracy_by_length.mean(), color='red', linestyle='--', label='Mean Accuracy')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{PROJECT_DIR}/figures/length_bias_analysis.png\", dpi=300)\n",
    "print(f\"\\n‚úì Length analysis saved to figures/length_bias_analysis.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.5: Error Analysis - What Does the Model Get Wrong?\n",
    "\n",
    "Systematic characterization of failure modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify errors\n",
    "errors_df = pd.DataFrame({\n",
    "    'premise': [val_examples[i]['premise'] for i in range(len(val_examples))],\n",
    "    'hypothesis': [val_examples[i]['hypothesis'] for i in range(len(val_examples))],\n",
    "    'true_label': [label_names[l] for l in true_labels],\n",
    "    'pred_label': [label_names[l] for l in baseline_pred_labels],\n",
    "    'correct': baseline_pred_labels == true_labels,\n",
    "    'overlap': overlaps,\n",
    "    'hyp_length': hyp_lengths\n",
    "})\n",
    "\n",
    "errors = errors_df[~errors_df['correct']]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ERROR ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTotal errors: {len(errors)} / {len(errors_df)} ({len(errors)/len(errors_df):.1%})\")\n",
    "print(f\"\\nError distribution by true label:\")\n",
    "print(errors['true_label'].value_counts())\n",
    "\n",
    "print(f\"\\nError distribution by predicted label:\")\n",
    "print(errors['pred_label'].value_counts())\n",
    "\n",
    "print(f\"\\nMost common error types (true ‚Üí predicted):\")\n",
    "error_types = errors.groupby(['true_label', 'pred_label']).size().sort_values(ascending=False)\n",
    "print(error_types.head(10))\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample representative errors\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAMPLE ERRORS (for qualitative analysis)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Sample errors from each category\n",
    "for true_label in label_names:\n",
    "    label_errors = errors[errors['true_label'] == true_label].sample(min(3, len(errors[errors['true_label'] == true_label])))\n",
    "    \n",
    "    print(f\"\\n{true_label.upper()} misclassified as:\")\n",
    "    for idx, row in label_errors.iterrows():\n",
    "        print(f\"\\n  Premise: {row['premise'][:100]}...\")\n",
    "        print(f\"  Hypothesis: {row['hypothesis']}\")\n",
    "        print(f\"  Predicted: {row['pred_label']} (overlap: {row['overlap']:.2f}, length: {row['hyp_length']})\")\n",
    "        print(\"  \" + \"-\"*50)\n",
    "\n",
    "print(\"\\n‚úì Sample errors displayed for qualitative analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.6: Summary Statistics for Report\n",
    "\n",
    "Aggregate all findings for inclusion in technical report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile comprehensive analysis summary\n",
    "analysis_summary = {\n",
    "    \"baseline_performance\": {\n",
    "        \"accuracy\": float(accuracy_score(true_labels, baseline_pred_labels)),\n",
    "        \"per_class_accuracy\": {\n",
    "            label_names[i]: float(accuracy_score(\n",
    "                true_labels[true_labels == i],\n",
    "                baseline_pred_labels[true_labels == i]\n",
    "            )) for i in range(3)\n",
    "        }\n",
    "    },\n",
    "    \"hypothesis_only_bias\": {\n",
    "        \"accuracy\": float(hyp_accuracy),\n",
    "        \"severity\": artifact_severity,\n",
    "        \"interpretation\": interpretation,\n",
    "        \"vs_random_baseline\": float(hyp_accuracy - 0.333),\n",
    "    },\n",
    "    \"lexical_overlap_bias\": {\n",
    "        \"mean_overlap\": float(overlaps.mean()),\n",
    "        \"std_overlap\": float(overlaps.std()),\n",
    "        \"accuracy_by_overlap\": accuracy_by_overlap.to_dict(),\n",
    "        \"correlation\": \"Strong\" if accuracy_by_overlap.iloc[-1] > accuracy_by_overlap.iloc[0] + 0.1 else \"Weak\"\n",
    "    },\n",
    "    \"length_bias\": {\n",
    "        \"mean_length\": float(hyp_lengths.mean()),\n",
    "        \"accuracy_by_length\": accuracy_by_length.to_dict(),\n",
    "    },\n",
    "    \"error_analysis\": {\n",
    "        \"total_errors\": int(len(errors)),\n",
    "        \"error_rate\": float(len(errors) / len(errors_df)),\n",
    "        \"errors_by_true_label\": errors['true_label'].value_counts().to_dict(),\n",
    "        \"top_error_types\": error_types.head(5).to_dict(),\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save comprehensive summary\n",
    "with open(f\"{PROJECT_DIR}/analysis_results/part1_analysis_summary.json\", 'w') as f:\n",
    "    json.dump(analysis_summary, f, indent=2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PART 1 ANALYSIS COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüìä Results Summary:\")\n",
    "print(f\"   Baseline Accuracy: {analysis_summary['baseline_performance']['accuracy']:.2%}\")\n",
    "print(f\"   Hypothesis-Only: {analysis_summary['hypothesis_only_bias']['accuracy']:.2%} ({artifact_severity})\")\n",
    "print(f\"   Lexical Overlap: {analysis_summary['lexical_overlap_bias']['correlation']} correlation\")\n",
    "print(f\"   Error Rate: {analysis_summary['error_analysis']['error_rate']:.1%}\")\n",
    "\n",
    "print(f\"\\nüìÅ Saved Files:\")\n",
    "print(f\"   {PROJECT_DIR}/analysis_results/part1_analysis_summary.json\")\n",
    "print(f\"   {PROJECT_DIR}/analysis_results/hypothesis_only_results.json\")\n",
    "print(f\"   {PROJECT_DIR}/figures/hypothesis_only_confusion_matrix.png\")\n",
    "print(f\"   {PROJECT_DIR}/figures/lexical_overlap_analysis.png\")\n",
    "print(f\"   {PROJECT_DIR}/figures/length_bias_analysis.png\")\n",
    "\n",
    "print(f\"\\nüìù For Technical Report:\")\n",
    "print(f\"   Section 3 (Analysis): Use findings from this notebook\")\n",
    "print(f\"   Figures: Include generated visualizations\")\n",
    "print(f\"   Tables: Use accuracy_by_overlap and accuracy_by_length\")\n",
    "\n",
    "print(f\"\\nüéØ Next Steps:\")\n",
    "print(f\"   ‚Üí Run Part 2: Mitigation (colab_mitigation_part2.ipynb)\")\n",
    "print(f\"   ‚Üí Implement debiasing methods based on identified artifacts\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Analysis Complete ‚úì\n",
    "\n",
    "### Key Findings:\n",
    "1. **Hypothesis-Only Bias**: Quantified severity of artifact exploitation\n",
    "2. **Lexical Overlap**: Tested correlation with model predictions\n",
    "3. **Length Bias**: Analyzed impact of hypothesis length\n",
    "4. **Error Patterns**: Identified systematic failure modes\n",
    "\n",
    "### For Technical Report (Part 1):\n",
    "- **Section 3.1**: Baseline performance and setup\n",
    "- **Section 3.2**: Hypothesis-only baseline analysis\n",
    "- **Section 3.3**: Lexical overlap and length bias\n",
    "- **Section 3.4**: Error characterization\n",
    "- **Figures**: Include all generated visualizations\n",
    "\n",
    "### Next: Part 2 - Mitigation\n",
    "Design and implement debiasing methods to address identified artifacts."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
